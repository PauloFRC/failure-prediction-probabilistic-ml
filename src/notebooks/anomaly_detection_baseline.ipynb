{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6315573b",
   "metadata": {},
   "source": [
    "## Analysis of the log sliding windows dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from utils.paths import project_root\n",
    "from dataloaders.sliding_window import LogsSlidingWindow\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a67e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Timestamp</th><th>EventId</th><th>Label</th><th>Anomaly</th></tr><tr><td>datetime[μs]</td><td>str</td><td>str</td><td>bool</td></tr></thead><tbody><tr><td>2005-06-03 22:42:50</td><td>&quot;3aa50e45&quot;</td><td>&quot;-&quot;</td><td>false</td></tr><tr><td>2005-06-03 22:42:50</td><td>&quot;3aa50e45&quot;</td><td>&quot;-&quot;</td><td>false</td></tr><tr><td>2005-06-03 22:42:50</td><td>&quot;3aa50e45&quot;</td><td>&quot;-&quot;</td><td>false</td></tr><tr><td>2005-06-03 22:42:50</td><td>&quot;3aa50e45&quot;</td><td>&quot;-&quot;</td><td>false</td></tr><tr><td>2005-06-03 22:42:50</td><td>&quot;3aa50e45&quot;</td><td>&quot;-&quot;</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────────────┬──────────┬───────┬─────────┐\n",
       "│ Timestamp           ┆ EventId  ┆ Label ┆ Anomaly │\n",
       "│ ---                 ┆ ---      ┆ ---   ┆ ---     │\n",
       "│ datetime[μs]        ┆ str      ┆ str   ┆ bool    │\n",
       "╞═════════════════════╪══════════╪═══════╪═════════╡\n",
       "│ 2005-06-03 22:42:50 ┆ 3aa50e45 ┆ -     ┆ false   │\n",
       "│ 2005-06-03 22:42:50 ┆ 3aa50e45 ┆ -     ┆ false   │\n",
       "│ 2005-06-03 22:42:50 ┆ 3aa50e45 ┆ -     ┆ false   │\n",
       "│ 2005-06-03 22:42:50 ┆ 3aa50e45 ┆ -     ┆ false   │\n",
       "│ 2005-06-03 22:42:50 ┆ 3aa50e45 ┆ -     ┆ false   │\n",
       "└─────────────────────┴──────────┴───────┴─────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(f\"{project_root()}/data/parsed/cleaned_BGL_structured.csv\")[['Timestamp', 'EventId', 'Label']]\n",
    "\n",
    "# filter only logs without anomaly labels \"-\"\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"Label\") != \"-\").alias(\"Anomaly\")\n",
    ")\n",
    "\n",
    "# convert Timestamp to polars Timestamp\n",
    "df = df.with_columns(\n",
    "    pl.from_epoch(pl.col(\"Timestamp\"), time_unit=\"s\").alias(\"Timestamp\")\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3acba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test spit\n",
    "split_point = int(len(df) * 0.7)\n",
    "train_df = df[:split_point]\n",
    "test_df = df[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d9201-7f99-452d-a5bc-feafc4347eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (42, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Label</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;KERNPOW&quot;</td><td>192</td></tr><tr><td>&quot;APPREAD&quot;</td><td>5983</td></tr><tr><td>&quot;KERNSERV&quot;</td><td>94</td></tr><tr><td>&quot;KERNSTOR&quot;</td><td>63491</td></tr><tr><td>&quot;KERNSOCK&quot;</td><td>209</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;MASABNORM&quot;</td><td>37</td></tr><tr><td>&quot;KERNRTSP&quot;</td><td>3983</td></tr><tr><td>&quot;KERNMC&quot;</td><td>342</td></tr><tr><td>&quot;KERNREC&quot;</td><td>6145</td></tr><tr><td>&quot;KERNMICRO&quot;</td><td>1503</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (42, 2)\n",
       "┌───────────┬───────┐\n",
       "│ Label     ┆ count │\n",
       "│ ---       ┆ ---   │\n",
       "│ str       ┆ u32   │\n",
       "╞═══════════╪═══════╡\n",
       "│ KERNPOW   ┆ 192   │\n",
       "│ APPREAD   ┆ 5983  │\n",
       "│ KERNSERV  ┆ 94    │\n",
       "│ KERNSTOR  ┆ 63491 │\n",
       "│ KERNSOCK  ┆ 209   │\n",
       "│ …         ┆ …     │\n",
       "│ MASABNORM ┆ 37    │\n",
       "│ KERNRTSP  ┆ 3983  │\n",
       "│ KERNMC    ┆ 342   │\n",
       "│ KERNREC   ┆ 6145  │\n",
       "│ KERNMICRO ┆ 1503  │\n",
       "└───────────┴───────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ee12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted 4713493 rows by timestamp\n",
      "Found 390 unique event types\n",
      "Period -> Start time: 2005-06-03 22:42:50, End time: 2006-01-04 16:00:05\n",
      "Generated 309193 sliding windows\n",
      "Window size: 5m, Step size: 1m\n",
      "Building index...\n",
      "Index built. Shape: (309193, 2)\n",
      "Isolation Forest: Removed 6184 windows (2.0%)\n",
      "Remaining windows: 303009\n",
      "Sorted 1414048 rows by timestamp\n",
      "Found 390 unique event types\n",
      "Period -> Start time: 2005-08-30 05:28:32, End time: 2006-01-04 16:00:05\n",
      "Generated 183507 sliding windows\n",
      "Window size: 5m, Step size: 1m\n",
      "Building index...\n",
      "Index built. Shape: (183507, 2)\n",
      "Train batches: 2368\n",
      "Test batches: 1434\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LogsSlidingWindow(\n",
    "    df, \n",
    "    window_size='5m',   \n",
    "    step_size='1m',\n",
    "    filter_strategy='isolation_forest',\n",
    "    filter_params={'contamination': 0.02}\n",
    ")\n",
    "\n",
    "train_event_ids = train_dataset.event_ids\n",
    "\n",
    "test_dataset = LogsSlidingWindow(\n",
    "    test_df,\n",
    "    window_size='5m',\n",
    "    step_size='1m', \n",
    "    event_ids=train_event_ids,  # use same event IDs as training set\n",
    "    filter_strategy='none'  # keep everything for evaluation\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1300270",
   "metadata": {},
   "source": [
    "### Simple Autoencoder Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64]):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        for i in range(len(hidden_dims) - 1, 0, -1):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i-1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(model, train_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    \n",
    "    print(\"\\nTraining Autoencoder...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for (data, _, _) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(data)\n",
    "            loss = criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(model, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            \n",
    "            reconstruction_error = criterion(reconstructed, data).mean(dim=1)\n",
    "            \n",
    "            all_scores.extend(reconstruction_error.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_scores), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 248.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.002993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 251.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.002043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 252.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.001772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 251.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.001644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 251.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.001555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 244.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.001510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:11<00:00, 211.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.001440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:10<00:00, 224.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.001384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 248.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.001348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 249.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.001325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 249.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.001288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 243.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.001284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:10<00:00, 216.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.001268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:10<00:00, 231.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.001237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 248.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.001237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 250.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.001229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 247.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.001209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 245.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.001212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 244.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.001195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|█████████████████████████████████████████████████████████████████| 2368/2368 [00:09<00:00, 243.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.001193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(train_event_ids)\n",
    "model = SimpleAutoencoder(input_dim=input_dim, hidden_dims=[256, 128, 64])\n",
    "\n",
    "train_losses = train_autoencoder(model, train_loader, epochs=20, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab224f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 1434/1434 [00:05<00:00, 281.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0691329e-03 1.0663269e-03 2.3738075e-06 ... 2.6157585e-03 2.3738075e-06\n",
      " 2.3738075e-06] \n",
      "\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "anomaly_scores, y_test = evaluate_autoencoder(model, test_loader)\n",
    "\n",
    "print(anomaly_scores, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bbecf5-ed3b-4a42-a65f-dff0a8077612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold (95th percentile): 0.001780\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        reconstructed = model(data)\n",
    "        reconstruction_error = nn.MSELoss(reduction='none')(reconstructed, data).mean(dim=1)\n",
    "        train_scores.extend(reconstruction_error.cpu().numpy())\n",
    "\n",
    "train_scores = np.array(train_scores)\n",
    "threshold = np.percentile(train_scores, 95)\n",
    "print(f\"Anomaly threshold (95th percentile): {threshold:.6f}\")\n",
    "\n",
    "y_pred = (anomaly_scores > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8af30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      0.94      0.97    180516\n",
      "     Anomaly       0.21      0.88      0.34      2991\n",
      "\n",
      "    accuracy                           0.94    183507\n",
      "   macro avg       0.60      0.91      0.65    183507\n",
      "weighted avg       0.99      0.94      0.96    183507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcfdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
